x-airflow-common: &airflow-common
  # image: your-repo/volka-airflow:latest # Use this if you build and push the image
  build:
    context: . # The build context is the project root.
    dockerfile: ./Dockerfile
  user: "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-0}" # Avoids permission issues with mounted volumes
  env_file:
    - .env # Load environment variables for DB connection, API keys etc.
  environment:
    - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
    - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
    # --- PostgreSQL Backend ---
    # These variables are constructed from the .env file.
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${DB_USER}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}/${DB_NAME}
    - AIRFLOW__CORE__SECRET_KEY=${AIRFLOW__CORE__SECRET_KEY}
    # --- Celery Executor Configuration ---
    - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
    - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${DB_USER}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}/${DB_NAME}
    # --- Custom Pipeline Configuration ---
    # Use these to switch between PostgreSQL and the Iceberg/DuckDB lakehouse.
    - ETL_SINK_TYPE=iceberg # Options: 'postgres' or 'iceberg'
    - DBT_TARGET_PROFILE=duckdb_iceberg # Options: 'dev' (for postgres) or 'duckdb_iceberg'
    - DBT_DUCKDB_PATH=/opt/airflow/dwh/volka_lakehouse.duckdb
    # --- Secrets Backend (Optional but good practice, ensure provider is installed if used) --- db+postgresql
    # - AIRFLOW__SECRETS__BACKEND=airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend
    # - AIRFLOW__SECRETS__BACKEND_KWARGS={"connections_prefix": "airflow/connections", "variables_prefix": "airflow/variables", "config_prefix": "airflow/config", "region_name": "eu-central-1"}
  volumes:
    - ./airflow_dags:/opt/airflow/dags
    - ./etl:/opt/airflow/etl # Mount ETL code for live changes
    - ./dbt_project:/opt/airflow/dbt_project # Mount dbt project for live changes
    - ./sql:/opt/airflow/sql # Mount SQL files if needed
    - ./logs:/opt/airflow/logs # Persist logs
    - ./dwh:/opt/airflow/dwh # Persist the DuckDB database file
    - ./plugins:/opt/airflow/plugins # Persist plugins
    - ./xlsx_excel_report:/opt/airflow/xlsx_excel_report # Persist report into xlxs_excel_output
  depends_on:
    postgres_db:
      condition: service_healthy
    redis:
      condition: service_healthy

services:
  # This service runs once on startup to initialize the database and create the admin user.
  airflow-init:
    <<: *airflow-common
    container_name: volka_airflow_init
    entrypoint: /bin/bash
    command:
      - -ec # Use -e to exit immediately if a command exits with a non-zero status.
      - |
        airflow db migrate # upgrade
        # Create admin user; `|| true` prevents the script from failing if the user already exists.
        airflow users create -u admin -p admin -r Admin -e admin@example.com -f admin -l user || true

  postgres_db:
    image: postgres:14
    container_name: volka_postgres_db
    ports: # Expose for local development/debugging
      - "5433:5432" # Expose Postgres on host port 5433 to avoid conflict
    environment:
      - POSTGRES_USER=${DB_USER:-airflow_user}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-airflow_pass}
      - POSTGRES_DB=${DB_NAME:-airflow_db}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    # This health check ensures that other services only start after the database is ready to accept connections.
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${DB_USER:-airflow_user}", "-d", "${DB_NAME:-airflow_db}"]
      interval: 5s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7.2-alpine
    container_name: volka_redis
    ports: # Expose for local development/debugging
      - "6379:6379"
    # This health check ensures Redis is responsive before Airflow components try to connect.
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 50 # Give Redis more time to start on some systems

  airflow-webserver:
    <<: *airflow-common
    container_name: volka_airflow_webserver
    command: ["webserver"] # This is the default command for the webserver service.
    ports: # Expose for local development/debugging
      - "8080:8080"
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8080/health"]
      interval: 30s
      timeout: 30s
      retries: 5

  airflow-scheduler:
    <<: *airflow-common
    container_name: volka_airflow_scheduler
    command: ["scheduler"]
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname)"]
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-worker:
    <<: *airflow-common
    container_name: volka_airflow_worker
    command:
      - bash
      - -c
      - "echo '--- Airflow Worker Environment Variables ---' && env | grep -E '^(DB_|AIRFLOW__)' | sort && echo '------------------------------------------' && airflow celery worker"
      # - "echo '--- Airflow Worker Environment Variables ---' && env | grep 'DB_\\|AIRFLOW__' | sort && echo '------------------------------------------' && airflow celery worker"    
    # command: ["celery", "worker"]
    # You can scale workers using: docker-compose up --scale airflow-worker=3
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type WorkerJob --hostname $(hostname)"]
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-triggerer:
    <<: *airflow-common
    container_name: volka_airflow_triggerer
    command: ["triggerer"]
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type TriggererJob --hostname $(hostname)"]
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-flower:
    <<: *airflow-common
    container_name: volka_airflow_flower
    command: ["celery", "flower", "--broker=redis://redis:6379/0"]
    ports: # Expose for local development/debugging
      - "5555:5555"

volumes:
  postgres_data:
  # redis_data: # Optional: if you want to persist Redis data